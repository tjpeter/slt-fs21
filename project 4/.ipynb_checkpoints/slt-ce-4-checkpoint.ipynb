{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "\n",
    "### Legi-Nr. 01-920-446\n",
    "    \n",
    "<u1/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLT-CE-4: Constant Shift Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many real-world phenomena are described by pairwise proximity data, modeling interactions between the entities of the system. This in contrast to the more common situation where each data sample is given as a feature vector. Even though the clustering of the proximity data may be performed directly on the data matrix, there are some advantatages of  embedding the data into a vector space. For example, it enables the use of some standard preprocessing techniques such as denoising or dimensionality reduction. In this coding exercise, we will explore the tecnhique called _Constant Shift Embedding_ for restating pairwise clustering problems in vector spaces [1] while preserving the cluster structure. We will apply the algorithm described in [1] to cluster the groups of research community members based on the email correspondence matrix. The data and its description is given in [2].\n",
    "\n",
    "### References \n",
    "\n",
    "[1] [Optimal cluster preserving embedding of nonmetric proximity data](https://ieeexplore.ieee.org/document/1251147)\n",
    "\n",
    "[2] [email-Eu-core](https://snap.stanford.edu/data/email-Eu-core.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"background-color:#f0b375;\"> Setup </h2>\n",
    "\n",
    "We start by importing necessary python packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#f1f8ff\"> \n",
    "For plots, we will use the seaborn module. Uncomment and run following code to install it (if necessary).\n",
    "<u1/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import matplotlib.pylab as plt\n",
    "import pylab\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import scipy.linalg as la\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import ticker\n",
    "\n",
    "# add addtional packages\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.utils.linear_assignment_ import linear_assignment\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Fix randoom seed for reproducibility\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes is hardcoded for simplicity (taken from [2]):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODES = 1005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the file which contains the list of interactions between the community members (nodes). Our data matrix represents an undirected graph which connects two nodes if there was at least one email sent between the two corresponding community members. Thus our data matrix is essentially an adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data matrix which will be an adjacency matrix\n",
    "DATA = np.zeros((NUM_NODES, NUM_NODES))\n",
    "\n",
    "# fill out the symmetric adjacency matrix\n",
    "with open(\"email-Eu-core.txt\") as file:\n",
    "    for line in file:\n",
    "        pair = [int(x) for x in line.split()]\n",
    "        DATA[pair[0], pair[1]] = 1\n",
    "        DATA[pair[1], pair[0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that DATA specifies an adjacency matrix of the email graph. It's not claimed to be a proper dissimilarity matrix required by CSE algorithm. So, you are allowed to perform any manipulations to construct a suitable (dis-)similarity matrix for the further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(DATA, cmap = 'Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "    \n",
    "We make a few checks of the dataset.\n",
    "1. Unique values and their class weights --> 0 and 1 only, with much more ones (15 to 0.5)\n",
    "2. diagonal elements --> 0,1 as well. Not a dissimilarty matrix\n",
    "3. column and row sums --> not zero --> not centralized    \n",
    "4. matrix is indeed symmetric\n",
    "    \n",
    "As we also have 1's on the diagonal, i.e. people sent e-mails to themselves, we first need to remove these to get a proper zero-diagonal dissimilarity matrix as required by CSE. \n",
    "</u1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('class weights:', compute_class_weight('balanced', np.unique(DATA), np.ravel(DATA)))\n",
    "print('diagonal: ', np.diagonal(DATA))\n",
    "print('column sum: ',np.sum(DATA, axis=0))\n",
    "print('row sum: ',np.sum(DATA, axis=1))\n",
    "print('symmetric T/F:',np.array_equal(DATA,DATA.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a class which contains main functionalities - TO BE IMPLEMENTED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantShiftEmbedding(skl.base.BaseEstimator, skl.base.TransformerMixin):\n",
    "    \"\"\"Template class for Constant Shift Embedding (CSE)\n",
    "    \n",
    "    Attributes:\n",
    "        PMAT (np.ndarray): Proximity matrix used for calculating the embeddings.\n",
    "        S (np.ndarray): Similarity matrix.\n",
    "        D (np.ndarray): Dissimilarity matrix.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose = False):\n",
    "        self.PMAT = None\n",
    "        self.S = None\n",
    "        self.D = None\n",
    "        \n",
    "        # Add/change parameters, if necessary.\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def preprocess(self, PMAT):\n",
    "        '''\n",
    "        Symmetrizes and removes diagonal elements.\n",
    "        '''\n",
    "        # symmetrize if necessary\n",
    "        if not np.all(np.abs(PMAT - PMAT.T) < 1e-4): \n",
    "            PMAT = (PMAT + PMAT.T)/2  \n",
    "        \n",
    "        # set diagonal to zero\n",
    "        np.fill_diagonal(PMAT, 0) \n",
    "\n",
    "        return PMAT\n",
    "    \n",
    "    def centralize(self, A):\n",
    "        '''\n",
    "        Computes the centralized matrix of a square matrix A\n",
    "        '''\n",
    "        n = A.shape[0]\n",
    "        Q = np.identity(n) - np.ones((n,n))/n\n",
    "        A_centr = Q @ A @ Q \n",
    "\n",
    "        return A_centr\n",
    "    \n",
    "    \n",
    "    def dijkstra(self, A): #, weight=0, margin=100):\n",
    "        '''\n",
    "           Calculate dissimiliarity matrix with shortest path of incidence matrix A with Dijkstra\n",
    "        '''\n",
    "        D = scipy.sparse.csgraph.dijkstra(A, directed=True, indices=None, return_predecessors=False, unweighted=False)                \n",
    "        \n",
    "        weight = self.weight\n",
    "        margin = self.margin\n",
    "        \n",
    "        # setting infty to a multiple of the max plus a margin\n",
    "        i_inf = (D == np.inf)\n",
    "        D[i_inf] = 0\n",
    "        D[i_inf] = weight*np.max(D) + margin\n",
    "        \n",
    "        assert np.all(np.abs(D - D.T) < 1e-4)\n",
    "        \n",
    "        return D   \n",
    "    \n",
    "    \n",
    "    def hamming(self, A):\n",
    "        '''\n",
    "           Calculate hamming distance of rows to obtain dissimilarity matrix\n",
    "        '''\n",
    "        \n",
    "        n = A.shape[0]\n",
    "        B = np.zeros( A.shape)\n",
    "        \n",
    "        # determine pairwise dissimilarities\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                x = A[i , :]\n",
    "                y = A[j, :]\n",
    "                # make vectors comparable by exchanging i and j entry of y\n",
    "                temp = y[i].copy()\n",
    "                y[i] = y[j]\n",
    "                y[j] = temp\n",
    "                \n",
    "                B[i, j] = scipy.spatial.distance.hamming(x,y)\n",
    "                B[j, i] = B[i, j]  # ensure symmetry\n",
    "                \n",
    "        return B\n",
    "    \n",
    "    def jaccard(self, A, const = 1):\n",
    "        '''\n",
    "           Calculate jaccard distance of rows to obtain dissimilarity matrix\n",
    "        '''\n",
    "        \n",
    "        n = A.shape[0]\n",
    "        B = np.zeros( A.shape)\n",
    "        \n",
    "        # determine pairwise dissimilarities\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                x = A[i , :]\n",
    "                y = A[j, :]\n",
    "                temp = y[i].copy()\n",
    "                y[i] = y[j]\n",
    "                y[j] = temp\n",
    "                 # make vectors comparable by exchanging i and j entry of y\n",
    "                intersection = np.logical_and(x, y)\n",
    "                union = np.logical_or(x, y)\n",
    "                if float(union.sum()) < 1e-4:\n",
    "                    B[i, j] = const \n",
    "                else: \n",
    "                    B[i, j] = intersection.sum() / float(union.sum())\n",
    "                B[j, i] = B[i, j]  # ensure symmetry\n",
    "                \n",
    "        assert np.all(np.abs(B - B.T) < 1e-4)\n",
    "        \n",
    "        return B\n",
    "                        \n",
    "    def cosine_dis(self, A, const = 1):\n",
    "        '''\n",
    "           Calculate cosine distance of rows to obtain dissimilarity matrix\n",
    "        '''\n",
    "        \n",
    "        n = A.shape[0]\n",
    "        B = np.zeros(A.shape)\n",
    "        \n",
    "        # determine pairwise dissimilarities\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                x = A[i , :]\n",
    "                y = A[j, :]\n",
    "                temp = y[i].copy()\n",
    "                y[i] = y[j]\n",
    "                y[j] = temp\n",
    "\n",
    "                 # make vectors comparable by exchanging i and j entry of y\n",
    "                if ( (np.max(x) > 1e-4) and (np.max(y) > 1e-4) ):\n",
    "                    cos_sim = scipy.spatial.distance.cosine(x, y)  \n",
    "                    B[i, j] = 1 - cos_sim\n",
    "                else:\n",
    "                    B[i, j] = const\n",
    "                  # transform into a dissimilarity measure\n",
    "                B[j, i] = B[i, j]  # ensure symmetry\n",
    "                \n",
    "        return B   \n",
    "    \n",
    "\n",
    "    \n",
    "    def dissimilarity(self, A, method):\n",
    "        \"\"\" Calculate dissimiliarity matrix based on the below methods. and all\n",
    "        the necessary variables for calculating the embeddings.\n",
    "        \n",
    "        Args:\n",
    "            A (np.ndarray): matrix on which upon to build the dissimilarity matrix D\n",
    "            mode: method to determine the dissimilarity matris\n",
    "                  \n",
    "                  'simple' - just taking PMAT, checking symmetry and setting diagonal to zero\n",
    "                  'dijksta' - calculating shortest path with dijkstra with (preprocessed) A as incidence matrix\n",
    "                  'hamming' - hamming distance of rows of (preprocessed) A \n",
    "                  'jaccard' - jaccard distance of rows of (preprocessed) A \n",
    "                  'cosine_dis' - cosine distance of rows of (preprocessed) A \n",
    "        \n",
    "       \"\"\"             \n",
    "        D = A.copy()\n",
    "        D = self.preprocess(D)\n",
    "        \n",
    "        if method == 'dijkstra':\n",
    "            D = self.dijkstra(D)\n",
    "        elif method == 'hamming':\n",
    "            D = self.hamming(D) \n",
    "        elif method == 'jaccard':\n",
    "            D = self.jaccard(D)\n",
    "        elif method == 'cosine_dis':\n",
    "            D = self.cosine_dis(D)\n",
    "        elif method == 'simple':\n",
    "            D = D\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Unknown method for dissimilarity matrix selected.')\n",
    "                          \n",
    "        return D\n",
    "                                          \n",
    "    \n",
    "    def fit(self, PMAT, method='hamming', weight=0, margin=100):\n",
    "        \"\"\" Calculate similarity/dissimiliarity matrix and all\n",
    "        the necessary variables for calculating the embeddings.\n",
    "        \n",
    "        Args:\n",
    "            PMAT (np.ndarray): proximity matrix\n",
    "            mode: method to determine the dissimilarity matris\n",
    "                  \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Save data\n",
    "\n",
    "        self.PMAT = PMAT\n",
    "        self.weight = weight\n",
    "        self.margin = margin\n",
    "        \n",
    "        assert (PMAT.shape[0] == PMAT.shape[1])  # check that it is a square matrix\n",
    "        n = PMAT.shape[0]\n",
    "\n",
    "        # preprocess PMAT/D to get a zero-diagonal dissimilarity matrix\n",
    "\n",
    "        PMAT_old = PMAT.copy()\n",
    "        D = self.dissimilarity(PMAT, method)\n",
    "\n",
    "        self.D = D\n",
    "\n",
    "        \n",
    "        # centralize \n",
    "        D_centr = self.centralize(D)\n",
    "\n",
    "\n",
    "        # determine S centralized \n",
    "        S_centr = -0.5 * D_centr\n",
    "\n",
    "        # get eigenvalues of S_centralized\n",
    "        eigenvals, _ = np.linalg.eigh(S_centr)\n",
    "        eigenval_min = np.min(eigenvals)\n",
    "        # determine S tilde by diagonal shift\n",
    "        S_tilde = S_centr - eigenval_min * np.identity(n)\n",
    "\n",
    "        S_tilde_centr = self.centralize(S_tilde)\n",
    "        self.S_tilde_centr = S_tilde_centr\n",
    "        \n",
    "        D_tilde = D - 2 * eigenval_min * (np.ones(n) -  np.identity(n))\n",
    "        D_tilde_centr =  self.centralize(D_tilde)\n",
    "        self.D_tilde_centr = D_tilde_centr\n",
    "        \n",
    "        # check that centralizations are consistent as expected\n",
    "        assert(np.sum(np.abs(D_tilde_centr + 2 * S_tilde_centr)) < 1e-4)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def get_embedded_vectors(self, p):\n",
    "        \"\"\"Return embeddings\n",
    "        \n",
    "        Args:\n",
    "            p (np.ndarray): cut-off value in eigenspectrum\n",
    "        \n",
    "        Returns:\n",
    "            \n",
    "            Xp (np.ndarray): embedded vectors\n",
    "            Dp (np.ndarray): dissimilarity matrix\n",
    "            Sp (np.ndarray): associated S matrix\n",
    "            eigenvals (np.ndarray): all eigenvalues\n",
    "            eigenvecs (np.ndarray): all eigenvectors\n",
    "        \"\"\"\n",
    "\n",
    "        eigenvals, eigenvecs = np.linalg.eigh(self.S_tilde_centr)\n",
    "        # numerical instability: get slightly negative eigenvalue --> set it to zero as matrix is PSD\n",
    "        eigenvals[eigenvals < 0] = 0\n",
    "        \n",
    "        assert np.max(np.abs(self.S_tilde_centr - eigenvecs @ np.diag(eigenvals) @ eigenvecs.T)) < 10e-4\n",
    "\n",
    "        sorted_index = np.argsort(eigenvals)[::-1]\n",
    "        eigenvals = eigenvals[sorted_index]\n",
    "        eigenvecs = eigenvecs[:, sorted_index]\n",
    "        \n",
    "        assert all(x >= y for x, y in zip(eigenvals, eigenvals[1:]))\n",
    "        \n",
    "        Vp = eigenvecs[:, :p]\n",
    "        Lp = np.diag(eigenvals[:p])\n",
    "        Xp = np.matmul(Vp, np.sqrt(Lp))\n",
    "        Sp = np.matmul(Xp, Xp.T)\n",
    "        Dp = np.zeros(Sp.shape)\n",
    "        \n",
    "        for i in range(Dp.shape[0]):\n",
    "            for j in range(Dp.shape[1]):\n",
    "                 Dp[i,j] = Sp[i,i] + Sp[j,j] - 2 * Sp[i,j]\n",
    "        \n",
    "    \n",
    "        return Xp, Dp, Sp, eigenvals, eigenvecs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 4.0 \n",
    "<span style=font-size:50%> Complete all problems in this section to get a pass on this exercise. </span>\n",
    "</h2>\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"background-color:#adebad;\">Describe briefly and consicely the model given in [1]. Explain the main steps of _Constant Shift Embedding_ algorithm. See <a href=\"https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\">markdown cheatsheet</a> for text editing.</p>\n",
    "\n",
    "\n",
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "    \n",
    "**Constant Shift Embedding**: \n",
    "\n",
    "Starting point is a (zero-diagonal) dissimilarity matrix $D$ (to be defined for the above data), which might not be a distance matrix (in a Euclidean vector space). The idea of **Constant Shift Embedding** is to apply multiple, elementary transformations in order to transform $D$ into another dissimilarity matrix $\\tilde{D}$, which can be represented as a distance matrix in an Euclidean space, while not affecting the cluster assignment based on the *pair-wise clustering* cost function $H^{pc}$. This transformation into an Euclidean space allows to treat pair-wise clustering as $k$-means clustering, and potentially even more important, to apply preprocessing, dimension reduction and denoising techniques, which were not available in a non-metric setting.\n",
    "\n",
    "The steps to obtain $\\tilde{D}$ from $D$ are as follows:\n",
    "1. **Centralization**: $S^{c} := -\\frac{1}{2}QSD,$ where $Q=\\mathbb{1}_{n}-\\frac{1}{n}e_n e_n^{\\intercal}$ (corresponds to the first two steps in Figure 1 of [1]).\n",
    "2. **Diagonal shift** by the minimal eigenvalue in order to attain positive semi-definiteness: $\\tilde{S}:=S^{c}-\\lambda_{min}(S^c)\\cdot\\mathbb{1}_n$. The matrix $S$ can be represented as a Gram matrix $X^\\intercal\\cdot X$ for some matrix $X$ with columns $x_1,\\ldots,x_n$ (note that we have exchanged the transpose in comparison to [1]). \n",
    "3. $\\tilde{D}_{i,j}:=\\tilde{S}_{ii}+\\tilde{S}_{jj}-2\\cdot \\tilde{S}_{ij}$, i.e. $\\tilde{D}=D-2\\lambda_{min}(S^c)(e_n e_n^{\\intercal}-\\mathbb{1}_{n})$ (**off-diagonal shift**)\n",
    "    \n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "    Implement Constant Shift Embedding. We start off by making an instance of the corresponding class.\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define and fit an instance with Dijkstra and one with Hamming distance\n",
    "CSE_dijk = ConstantShiftEmbedding()  \n",
    "CSE_hamm = ConstantShiftEmbedding()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "    Fit the data matrix. _fit(...)_ method computes necessary variables which can be later on used to produce embeddings [1].\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSE_dijk.fit(DATA, method='dijkstra')\n",
    "CSE_hamm.fit(DATA, method='hamming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 4.5 \n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 4.5 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    Next, try to find approximately optimal $p = p^∗$, a cut-off value which removes noise from the data. To do that, produce an eigen-spectrum plot as shown in [1] figure 4a and briefly explain your choice of $p^∗$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "    In the following, we compute the eigenspectrum for the various methods. We plot for each method the eigenspectrum as well as two excerpts of it next to it.\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define axis-ranges for the subsequent plots\n",
    "axis_dict_1 = {'dijkstra': [[0,200], [25,100]], \n",
    "               'hamming': [[0,100], [0,5]], \n",
    "               'jaccard': [[0,200], [10,12]], \n",
    "               'cosine_dis': [[0,400], [15,18]], \n",
    "               'simple': [[0,400], [25,30]]\n",
    "            }\n",
    "\n",
    "axis_dict_2 = {'dijkstra': [[0,100], [30,60]], \n",
    "               'hamming': [[0,50], [0,2]], \n",
    "               'jaccard': [[0,100], [10,12]], \n",
    "               'cosine_dis': [[0,250], [15,18]], \n",
    "               'simple': [[0,250], [25,30]]\n",
    "            }\n",
    "\n",
    "METHODS = ['dijkstra', 'hamming', 'jaccard', 'cosine_dis', 'simple']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = DATA.shape[0]\n",
    "evs_index = np.arange(n)\n",
    "\n",
    "for method in METHODS :\n",
    "    \n",
    "    # for Dijkstra and Hamming already fitted above\n",
    "    if method == 'dijkstra':\n",
    "        CSE_mthd = CSE_dijk\n",
    "    elif method == 'hamming':\n",
    "        CSE_mthd = CSE_hamm\n",
    "    elif method == 'jaccard':\n",
    "        CSE_jacc = ConstantShiftEmbedding()\n",
    "        CSE_jacc.fit(DATA, method=method)   \n",
    "        CSE_mthd = CSE_jacc\n",
    "    elif method == 'cosine_dis':\n",
    "        CSE_cos = ConstantShiftEmbedding()\n",
    "        CSE_cos.fit(DATA, method=method)   \n",
    "        CSE_mthd = CSE_cos\n",
    "    elif method == 'simple':\n",
    "        CSE_sim = ConstantShiftEmbedding()\n",
    "        CSE_sim.fit(DATA, method=method)   \n",
    "        CSE_mthd = CSE_sim\n",
    "        \n",
    "    _, _, _, eigenvals, _  = CSE_mthd.get_embedded_vectors(n)\n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.suptitle(method + ' distance', fontsize=16)\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.plot(evs_index, eigenvals)\n",
    "    plt.xlabel('eigenvalue index')\n",
    "    plt.ylabel('eigenvalue')\n",
    "\n",
    "    plt.subplot(132)   \n",
    "    xs, ys = axis_dict_1[method]\n",
    "    plt.plot(evs_index, eigenvals)\n",
    "    plt.xlabel('eigenvalue index')\n",
    "    plt.ylabel('eigenvalue')\n",
    "    plt.xlim(xs)\n",
    "    plt.ylim(ys)\n",
    "\n",
    "    plt.subplot(133)\n",
    "    xs, ys = axis_dict_2[method]\n",
    "    plt.plot(evs_index, eigenvals)\n",
    "    plt.xlabel('eigenvalue index')\n",
    "    plt.ylabel('eigenvalue')\n",
    "    plt.xlim(xs)\n",
    "    plt.ylim(ys)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "With the exception of the Dijkstra and the Hamming distance, there are very long plateaus until the magnitude of the eigenvalues drops significantly (again). As the corresponding eigenvectors of these \"plateau eigenvalues\" cannot be really preferred over each other, these distances do not seem to be suited to reasonably reduce the dimension $p$. Nevertheless, we below picked some $p_{opt}$ for them as well. But otherwise, we will not use these methods any further.\n",
    "    \n",
    "For Dijkstra and Hamming loss we pick $p_{opt}$ to be 75 resp. 25,  at which point there already seems a transition into the plateau. That is, we are rather conservative here in the sense that we choose $p$ rather at the end of the elbow as in the middle of it, which is often common.\n",
    "\n",
    "    \n",
    "</u1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popt_dict = {'dijkstra': 75, \n",
    "             'hamming': 25, \n",
    "             'jaccard': 50, \n",
    "             'cosine_dis': 200, \n",
    "             'simple': 200\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in METHODS:\n",
    "    p_opt = popt_dict[method]\n",
    "    print(\"Chosen cut-off value for {} method is: {}\".format(method, p_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = ['dijkstra', 'hamming', 'jaccard', 'cosine_dis', 'simple']       \n",
    "\n",
    "n = DATA.shape[0]\n",
    "evs_index = np.arange(n)\n",
    "\n",
    "for method in METHODS:\n",
    "    \n",
    "    # select number of features\n",
    "    p_opt = popt_dict[method]\n",
    "\n",
    "    # for Dijkstra and Hamming already fitted above\n",
    "    if method == 'dijkstra':\n",
    "        CSE_mthd = CSE_dijk\n",
    "    elif method == 'hamming':\n",
    "        CSE_mthd = CSE_hamm\n",
    "    elif method == 'jaccard':\n",
    "        CSE_mthd = CSE_jacc\n",
    "    elif method == 'cosine_dis':\n",
    "        CSE_mthd = CSE_cos\n",
    "    elif method == 'simple':\n",
    "        CSE_mthd = CSE_sim\n",
    "\n",
    "    _, _, _, eigenvals, _  = CSE_mthd.get_embedded_vectors(n)\n",
    "    \n",
    "    plt.figure(figsize=(15,4))\n",
    "    plt.suptitle(method + ' distance', fontsize=16)\n",
    "\n",
    "    plt.subplot(131)\n",
    "    plt.plot(evs_index, eigenvals)\n",
    "    plt.axvline(p_opt-1, color='r', linestyle ='--')\n",
    "    plt.axhline(eigenvals[p_opt-1], color='r', linestyle ='--')\n",
    "    plt.xlabel('eigenvalue index')\n",
    "    plt.ylabel('eigenvalue')\n",
    "\n",
    "    plt.subplot(132)\n",
    "    xs, ys = axis_dict_1[method]\n",
    "    plt.plot(evs_index, eigenvals)\n",
    "    plt.axvline(p_opt-1, color='r', linestyle ='--')\n",
    "    plt.axhline(eigenvals[p_opt-1], color='r', linestyle ='--')\n",
    "    plt.xlabel('eigenvalue index')\n",
    "    plt.ylabel('eigenvalue')\n",
    "    plt.xlim(xs)\n",
    "    plt.ylim(ys)\n",
    "\n",
    "    plt.subplot(133)\n",
    "    xs, ys = axis_dict_2[method]\n",
    "    plt.plot(evs_index, eigenvals)\n",
    "    plt.axvline(p_opt-1, color='r', linestyle ='--')\n",
    "    plt.axhline(eigenvals[p_opt-1], color='r', linestyle ='--')\n",
    "    plt.xlabel('eigenvalue index')\n",
    "    plt.ylabel('eigenvalue')\n",
    "    plt.xlim(xs)\n",
    "    plt.ylim(ys)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 5.0 \n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 5.0 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    Plot the distance matrices both for the denoised ($p = p^*$ -- from the previous step) and the original versions as shown in figure 5 in [1]. Note that the distance matrix is a matrix with pairwise distances between every two points from the dataset ($d_{ij} = dist(x_i, x_j)$).<br>\n",
    "    Perform K-MEANS algorithm for varying number of clusters K on the embedded vectors derrived from CSE. You may use the sklearn implementation of K-MEANS. To make the aforementioned plots meaningful, sort the nodes according to the cluster belongings for every number of clusters K (see the figure 5). For now, there is no need to include the actual ground truth labels given in [2].\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "Based on the above analysis, we will perform this task sepearately for Dijkstra and Hamming distance. As such, we need to re-fit CSE again.\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hamm = popt_dict['hamming']\n",
    "p_dijk = popt_dict['dijkstra']\n",
    "n = DATA.shape[0]\n",
    "\n",
    "Xp_hamm_opt, Dp_hamm_opt, Sp_hamm_opt, _, _ = CSE_hamm.get_embedded_vectors(p_hamm)\n",
    "Xp_hamm_all, Dp_hamm_all, Sp_hamm_all, _, _ = CSE_hamm.get_embedded_vectors(n)\n",
    "\n",
    "Xp_dijk_opt, Dp_dijk_opt, Sp_dijk_opt, _, _ = CSE_dijk.get_embedded_vectors(p_dijk)\n",
    "Xp_dijk_all, Dp_dijk_all, Sp_dijk_all, _, _ = CSE_dijk.get_embedded_vectors(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "In the following we define a few helper functions for matching cluster labels, sorting data based on cluster labels and plotting corresponding heatmaps of distance matrices.\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_matching(c1, c2):\n",
    "    '''\n",
    "    permutate cluster labels of c2 to match clusterings optimally with c1. \n",
    "    Returns c2_perm\n",
    "    '''\n",
    "    conf_mat = confusion_matrix(c1, c2)\n",
    "    lin_assign = linear_assignment(-conf_mat.T)  # linear assignment minimizes, we want to maximize --> minus\n",
    "    \n",
    "    # create a replacement dictionarry\n",
    "    dic = {a : b for a,b in list(lin_assign)}\n",
    "    \n",
    "    k = np.array(list(dic.keys()))\n",
    "    v = np.array(list(dic.values()))\n",
    "\n",
    "    c2_perm = np.zeros_like(c2)\n",
    "    for key,val in zip(k,v):\n",
    "        c2_perm[c2==key] = val\n",
    "\n",
    "    return c2_perm, lin_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_indices(c, D=None):\n",
    "    '''\n",
    "     Args:\n",
    "            c : cluster assignment\n",
    "            D (optional): dissimilarity matrix\n",
    "        \n",
    "    Returns:\n",
    "            permutation: to order cluster labels\n",
    "            D_perm (optional): corresponding permutated matrix D\n",
    "    \n",
    "    '''\n",
    "    clusters = np.sort(np.unique(c))\n",
    "    cluster_sizes = [len(np.where(c == k)[0]) for k in clusters]\n",
    "\n",
    "    assert np.sum(cluster_sizes) == len(c)\n",
    "    \n",
    "    s = [np.where(c == k)[0].tolist() for k in clusters]\n",
    "    permutation = [item for sublist in s for item in sublist]\n",
    "    \n",
    "    if D is not None:\n",
    "        D_perm = D.copy()\n",
    "        D_perm = D[permutation,:]\n",
    "        D_perm = D[:, permutation]\n",
    "        return permutation , cluster_sizes, D_perm\n",
    "\n",
    "    return permutation , cluster_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmaps(D, Dp, cluster_sizes = None, cluster_sizes_p = None, vmin = 0, vmax = 1, k = None, cmap = 'Blues'):\n",
    "    '''\n",
    "    plots heatmaps of dissimillarity matrices D and Dp next to each other\n",
    "    Adds vertical and horizontal lines indicating the respective cluster\n",
    "    '''\n",
    "\n",
    "    f,(ax1,ax2, axcb) = plt.subplots(1,3, \n",
    "                gridspec_kw={'width_ratios':[1,1,0.1]}, figsize=(10, 4))\n",
    "    ax1.get_shared_y_axes().join(ax2)\n",
    "\n",
    "    g1 = sns.heatmap(D, cbar=False, ax=ax1, vmin=vmin, vmax=vmax, cmap=cmap) #, cmap='Blues', annot=True, fmt='') \n",
    "    g1.set_ylabel('')\n",
    "    g1.set_xlabel('')\n",
    "    g1.set_title('initial dissimilarity matrix', fontsize=14)\n",
    "\n",
    "    g2 = sns.heatmap(Dp, cbar_ax=axcb, ax=ax2, vmin=vmin, vmax=vmax, cmap=cmap) #, cmap='Blues', annot=True, fmt='') \n",
    "    g2.set_ylabel('')\n",
    "    g2.set_xlabel('')\n",
    "    g2.set_yticks([])\n",
    "    g2.set_title('denoised dissimilarity matrix', fontsize=14)\n",
    "    \n",
    "    if k is not None:\n",
    "        f.suptitle('{} clusters'.format(k), fontweight =\"bold\", fontsize=20, y =1.05)\n",
    "        f.tight_layout()\n",
    "        f.subplots_adjust(top=0.85)\n",
    "\n",
    "    if cluster_sizes is not None:\n",
    "        ax1.hlines(np.cumsum(cluster_sizes)[:-1], *ax1.get_xlim(), colors='red',linewidth=2)\n",
    "        ax1.vlines(np.cumsum(cluster_sizes)[:-1], *ax1.get_ylim(), colors='red',linewidth=2)\n",
    "\n",
    "    if cluster_sizes_p is not None:\n",
    "        ax2.hlines(np.cumsum(cluster_sizes_p)[:-1], *ax2.get_xlim(), colors='red',linewidth=2)\n",
    "        ax2.vlines(np.cumsum(cluster_sizes_p)[:-1], *ax2.get_ylim(), colors='red',linewidth=2)\n",
    "\n",
    "\n",
    "    for ax in [g1,g2]:\n",
    "        tl = ax.get_xticklabels()\n",
    "        ax.set_xticklabels(tl, rotation=90)\n",
    "        tly = ax.get_yticklabels()\n",
    "        ax.set_yticklabels(tly, rotation=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select different numbers of clusters for subsequent analysis\n",
    "K_SET = [2,3,5,7,10,20,30,42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "For various $k$, we plot the heatmaps of the permutated <b>Hamming</b> distance matrix: on the left for all features (<b>un-denoised</b>) and on the right for our selected $p_{opt}$ (<b>denoised</b>). We also indicate the cluster (sizes) in the plot until k=10, as for larger k it would become visually distracting\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMAP = plt.cm.binary\n",
    "\n",
    "for k in K_SET:\n",
    "    # fit k-means\n",
    "    kmeans_orig = KMeans(n_clusters=k, random_state=23, n_init=10).fit(Xp_hamm_all)\n",
    "    kmeans_popt = KMeans(n_clusters=k, random_state=23, n_init=10).fit(Xp_hamm_opt)\n",
    "    \n",
    "    # predict clusters\n",
    "    clusters_all = kmeans_orig.predict(Xp_hamm_all)\n",
    "    clusters_popt = kmeans_popt.predict(Xp_hamm_opt)\n",
    "    \n",
    "    # permutate/match cluster labels\n",
    "    clusters_popt_perm, _ = cluster_matching(clusters_all, clusters_popt)\n",
    "    \n",
    "    # sort datapoints such that cluster labels are in order and determine distance matrices\n",
    "    permutation , cluster_sizes , D_hamm_all_perm = sort_indices(clusters_all, Dp_hamm_all)\n",
    "    permutation_p , cluster_sizes_p, Dp_hamm_opt_perm  = sort_indices(clusters_popt_perm, Dp_hamm_opt)\n",
    "    \n",
    "    vmin = np.min(D_hamm_all_perm)\n",
    "    vmax = np.max(D_hamm_all_perm)\n",
    "    \n",
    "    # plot resulting distance matrices\n",
    "    if k > 10:  # to declutter diagram remove lines indicating clusters if there are too many\n",
    "        cluster_sizes = None\n",
    "        cluster_sizes_p = None\n",
    "    plot_heatmaps(D_hamm_all_perm, Dp_hamm_opt_perm, cluster_sizes, cluster_sizes_p, vmin, vmax, k, cmap = CMAP)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dijkstra distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "For various $k$, we plot the heatmaps of the permutated <b>Dijkstra</b> distance matrix: on the left for all features (<b>un-denoised</b>) and on the right for our selected $p_{opt}$ (<b>denoised</b>). We also indicate the cluster (sizes) in the plot until k=10, as for larger k it would become visually distracting\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CMAP = plt.cm.binary\n",
    "\n",
    "for k in K_SET:\n",
    "    # fit k-means\n",
    "    kmeans_orig = KMeans(n_clusters=k, random_state=23, n_init=10).fit(Xp_dijk_all)\n",
    "    kmeans_popt = KMeans(n_clusters=k, random_state=23, n_init=10).fit(Xp_dijk_opt)\n",
    "        \n",
    "    # predict clusters\n",
    "    clusters_all = kmeans_orig.predict(Xp_dijk_all)\n",
    "    clusters_popt = kmeans_popt.predict(Xp_dijk_opt)\n",
    "\n",
    "    # permutate/match cluster labels\n",
    "    clusters_popt_perm, _ = cluster_matching(clusters_all, clusters_popt)\n",
    "    \n",
    "    # sort datapoints such that cluster labels are in order and determine distance matrices\n",
    "    permutation , cluster_sizes , D_dijk_all_perm = sort_indices(clusters_all, Dp_dijk_all)\n",
    "    permutation_p , cluster_sizes_p, Dp_dijk_opt_perm  = sort_indices(clusters_popt_perm, Dp_dijk_opt)\n",
    "\n",
    "    vmin = np.min(D_dijk_all_perm)\n",
    "    vmax = np.max(D_dijk_all_perm)\n",
    "    \n",
    "    # plot resulting distance matrices\n",
    "    if k > 10:  # to declutter diagram remove lines indicating clusters if there are too many\n",
    "        cluster_sizes = None\n",
    "        cluster_sizes_p = None\n",
    "        \n",
    "    plot_heatmaps(D_dijk_all_perm, Dp_dijk_opt_perm, cluster_sizes, cluster_sizes_p, vmin, vmax, k, cmap=CMAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "Generally, the Hamming distance matrices show a clearer, visible recognizable pattern of the various clusters. There seems to be a more uniform distribution of the distances in comparison to the Dijkstra distance matrix, where distances seem to be much more concentrated within their range. This is especially reflected for the denoised Dijkstra case for smaller k, where there is one huge cluster and few very tiny ones. This could suggest that at least for the visual representation some scale transformation should be considered for the Dijkstra distances or that the parameter (currently at 100), which is set for inifinite distances should be tuned.\n",
    "Moreover, the process of denosing appears to be more robust for the Hamming distance in comparison to Dijkstra in the sense that cluster (sizes) change much more when denoising (for small k) for Dijkstra than Hamming. Depending on the situation, in particular the noise to signal ratio, this might be more or less desirable. We will later see how this plays out here in terms of accuracy.\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms to distance distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.suptitle('Hamming distance', fontsize=20)\n",
    "\n",
    "plt.subplot(121)\n",
    "D = Dp_hamm_all\n",
    "plt.hist(np.ravel(D[np.tril_indices(D.shape[0],k=-1)]), bins=50, edgecolor='black', linewidth=1.2)\n",
    "plt.title('un-denoised', fontsize=14)\n",
    "plt.subplot(122)\n",
    "D = Dp_hamm_opt\n",
    "plt.hist(np.ravel(D[np.tril_indices(D.shape[0],k=-1)]), bins=50, edgecolor='black', linewidth=1.2)\n",
    "plt.title('denoised', fontsize=14)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.suptitle('Dijkstra distance', fontsize=20)\n",
    "\n",
    "plt.subplot(121)\n",
    "D = Dp_dijk_all\n",
    "plt.hist(np.ravel(D[np.tril_indices(D.shape[0],k=-1)]), bins=50, edgecolor='black', linewidth=1.2)\n",
    "plt.title('un-denoised', fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "D = Dp_dijk_opt\n",
    "plt.hist(np.ravel(D[np.tril_indices(D.shape[0],k=-1)]), bins=50, edgecolor='black', linewidth=1.2)\n",
    "plt.title('denoised', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "The above histograms confirm are presumption that the Hamming distance is more evenly distributed compared to Dijkstra. The latter posseses one big peak at around a distance 60 for the un-denoised version resp. at around 5 for the denoised one and a smaller peak - corresponding to \"infinite\" distances - at around 160 resp. 150.    \n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 5.5 \n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 5.5 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "    Producing 2D and 3D embeddings allows us to nicely visualize generated clusters. Now calculate the embeddings for p = 2 (2D case) and p = 3 (3D case) and plot clusterings for a few values of K.  Alternatively, you could use $p = p^*$ for more gentle denoising, cluster the denoised embeddings and only then apply a dimensionality reduction technique to get a plot in 2,3-dimensional space. You could use PCA, LLE, t-SNE etc. figure out what works for you. As an example see figure 6 (b) from [1] where CSE is combined with PCA.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(X_fit, X_emb, k_list, dim=2):\n",
    "    '''\n",
    "    Fits k-means to D_fit for k in k_list\n",
    "    Plots then 2-dim resp. 3-dim embeddings of D_emb as scatter plots \n",
    "    marking the different clusters of the previously performed k-means\n",
    "    '''\n",
    "\n",
    "    if dim == 2:\n",
    "        figsize = (15,4)    \n",
    "    elif dim == 3:\n",
    "        figsize = (15,4)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Dimension needs to be 2 or 3.') \n",
    "        \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    \n",
    "    for cid, k in enumerate(k_list):\n",
    "        if dim == 2:\n",
    "            ax = fig.add_subplot(1, len(k_list), cid+1)\n",
    "        elif dim == 3:\n",
    "            ax = fig.add_subplot(1, len(k_list), cid+1, projection='3d')\n",
    "        else:\n",
    "            raise ValueError('Dimension needs to be 2 or 3.')\n",
    "        \n",
    "        ax.set_title('k = {}'.format(k),fontsize=15)\n",
    "        \n",
    "        kmeans = KMeans(n_clusters=k, random_state=23, n_init=10).fit(X_fit)\n",
    "        preds = kmeans.predict(X_fit)\n",
    "        \n",
    "        for c in range(k): # create scatter plots for clusters\n",
    "            mask = np.where(preds == c)\n",
    "            if dim == 2:\n",
    "                ax.scatter(X_emb[mask,0], X_emb[mask,1])\n",
    "            if dim == 3:\n",
    "                ax.scatter(X_emb[mask,0], X_emb[mask,1], X_emb[mask,2])\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "In the following, we will fit k-means again to the full as well as denoised distance matrix. We will then use following embeddings a) first 2 resp. 3 embedded vectors b) TSNE embedding. This will be done for both - Hamming and Djikstra loss. In Summary, we have following setting:\n",
    "\n",
    "* loss: Hamming, Dijkstra\n",
    "* matrix to fit k-means: D_all or D_popt\n",
    "* embedding dimension: 2,3\n",
    "* embedding method: first 2 resp. 3 embedded vectors\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded vectors dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 2\n",
    "X_fit = Xp_hamm_all\n",
    "X_emb, _,  _, _, _ = CSE_hamm.get_embedded_vectors(2)\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## dimension 2\n",
    "X_fit = Xp_hamm_opt\n",
    "X_emb, _,  _, _, _ = CSE_hamm.get_embedded_vectors(2)\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded vectors dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 3\n",
    "%matplotlib inline \n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "X_fit = Xp_hamm_all\n",
    "X_emb, _,  _, _, _ = CSE_hamm.get_embedded_vectors(3)\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 3\n",
    "%matplotlib inline \n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "X_fit = Xp_hamm_opt\n",
    "X_emb, _,  _, _, _ = CSE_hamm.get_embedded_vectors(3)\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "Using the first two resp. three embedding vectors for Hamming loss, we observe a rather good separation for smaller k. However, we do not see any significant effect from denoising, which was not expected. \n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 2\n",
    "X_fit = Xp_hamm_all\n",
    "X_emb = TSNE(n_components=2, random_state=23).fit_transform(X_fit)\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 2\n",
    "X_fit = Xp_hamm_opt\n",
    "X_emb = TSNE(n_components=2, random_state=23).fit_transform(X_fit)\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 3\n",
    "%matplotlib inline \n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "X_fit = Xp_hamm_all\n",
    "X_emb = TSNE(n_components=3, random_state=23).fit_transform(X_fit)\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 3\n",
    "%matplotlib inline \n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "X_fit = Xp_hamm_opt\n",
    "X_emb = TSNE(n_components=3, random_state=23).fit_transform(X_fit)\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "With TSNE, cluster separation for Hamming loss seems quite bad for undenoised data. With the denoised data, the visual separation (in particular for 2D) is significantly better. Yet, the separation pattern is rather of radial form: there is a large cluster in the center and smaller ones further out. \n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dijkstra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded vectors dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 2\n",
    "X_fit = Xp_dijk_all\n",
    "X_emb, _,  _, _, _ = CSE_dijk.get_embedded_vectors(2)\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 2\n",
    "X_fit = Xp_dijk_opt\n",
    "X_emb, _,  _, _, _ = CSE_dijk.get_embedded_vectors(2)\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded vectors dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 3\n",
    "%matplotlib inline \n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "X_fit = Xp_dijk_all\n",
    "X_emb, _,  _, _, _ = CSE_dijk.get_embedded_vectors(3)\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 3\n",
    "%matplotlib inline \n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "X_fit = Xp_dijk_opt\n",
    "X_emb, _,  _, _, _ = CSE_dijk.get_embedded_vectors(3)\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "With Dijkstra loss, (visual) cluster separation using embedded vectors does not work. We again observe a high cluster imbalance, concretely a dominating cluster for smaller k.\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE dim = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 2\n",
    "X_fit = Xp_dijk_all\n",
    "X_emb = TSNE(n_components=2, random_state=23).fit_transform(X_fit)\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "With Dijkstra loss and un-denoised data, TSNE does not separate clusters well. In particular for larger k, there is no clear pattern.\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 2\n",
    "X_fit = Xp_dijk_opt\n",
    "X_emb = TSNE(n_components=2, random_state=23).fit_transform(X_fit)\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=2, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "With Dijkstra loss and denoised data, TSNE does now actually separate clusters quite well. In particular for large k, we visually see clusters in contrast to the un-denoised case. These observations are similar in the 3D-case below.\n",
    "    \n",
    "Comparing the TSNE-embedded plots with the Hamming distance, we suspect that for higher k (in particular the true k=42), Dijkstra distance will form more accurate clusters than Hamming distance (tbd).\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 3\n",
    "%matplotlib inline \n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "X_fit = Xp_dijk_all\n",
    "X_emb = TSNE(n_components=3, random_state=23).fit_transform(X_fit)\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dimension 3\n",
    "%matplotlib inline \n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}\n",
    "\n",
    "X_fit = Xp_dijk_opt\n",
    "X_emb = TSNE(n_components=3, random_state=23).fit_transform(X_fit)\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[2,3,4])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[5,7,10])\n",
    "embedding(X_fit, X_emb, dim=3, k_list=[20,30,42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "Subsequently, we see that for the Dijkstra distance the largest eigenvalue dominates much more than in the Hamming case. For Dijkstra, this might cause the observed cluster imbalance and the fact that embeddings with the embedded vectors were not capable to separate clusters (visually).\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_, evals_dijk, _ = CSE_dijk.get_embedded_vectors(3)\n",
    "_,_,_, evals_hamm, _ = CSE_hamm.get_embedded_vectors(3)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.suptitle('Eigenvalues', fontsize=20)\n",
    "\n",
    "plt.subplot(121)\n",
    "D = Dp_hamm_all\n",
    "plt.bar(x=np.arange(0,50),height=evals_dijk[:50])\n",
    "plt.title('Dijkstra', fontsize=14)\n",
    "plt.subplot(122)\n",
    "plt.bar(x=np.arange(0,50),height=evals_hamm[:50])\n",
    "plt.title('Hamming', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:#f0b375;\">\n",
    "Section 6.0 \n",
    "<span style=font-size:50%> Complete all problems in this and previous sections to get a grade of 6.0 </span>\n",
    "</h2>\n",
    "\n",
    "<p style=\"background-color:#adebad;\">\n",
    "Finally, to evaluate the quality of the above derived clusters, let's compare our predictions with the ground truth. We will use the actual member-institution mappings given in [2]. You can reuse code from the previous coding exercises to align the cluster labels with the ground truth.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize community members affeliation array\n",
    "AFFILIATIONS = np.zeros((NUM_NODES, ))\n",
    "\n",
    "# Fill out the affiliation array\n",
    "with open(\"email-Eu-core-department-labels.txt\") as file:\n",
    "    for line in file:\n",
    "        pair = [int(x) for x in line.split()]\n",
    "        AFFILIATIONS[pair[0]] = pair[1]\n",
    "\n",
    "# Number of organizations is \n",
    "print(\"The true number of clusters (departments) is: \",len(np.unique(AFFILIATIONS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFFILIATIONS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(X_emb, y_true, y_pred, k, method, accuracy):\n",
    "    '''\n",
    "    two plots of the embedding X_emb next to each other:\n",
    "    on the left with cluster coloring according to y_true\n",
    "    on the right according to y_pred\n",
    "    '''\n",
    "    plt.figure(figsize=(20,6))\n",
    "    plt.tight_layout(pad=2)\n",
    "    plt.suptitle('{} with k={}, Accuracy={:.2f}'.format(method,k,accuracy), fontsize=20)\n",
    "    plt.subplots_adjust(top=0.82)\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.title('ground truth', fontsize=15, y=1.02)\n",
    "    for c in range(k):\n",
    "        mask = np.where(y_true == c)\n",
    "        plt.scatter(X_emb[mask, 0], X_emb[mask, 1])    \n",
    "        \n",
    "    plt.subplot(122)\n",
    "    plt.title('predictions', fontsize=15,  y=1.02)\n",
    "    for c in range(k):\n",
    "        mask = np.where(y_pred == c)\n",
    "        plt.scatter(X_emb[mask, 0], X_emb[mask, 1]) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "k = len(np.unique(AFFILIATIONS))\n",
    "y_true = AFFILIATIONS\n",
    "\n",
    "for method in ['dijkstra', 'hamming']:\n",
    "    p_opt = popt_dict[method]\n",
    "\n",
    "    if method == 'dijkstra':\n",
    "        CSE_mthd = CSE_dijk\n",
    "    elif method == 'hamming':\n",
    "        CSE_mthd = CSE_hamm\n",
    "\n",
    "    X_popt, D_popt, _, _, _   = CSE_mthd.get_embedded_vectors(p_opt)\n",
    "    # TSNE embedding\n",
    "    X_emb = TSNE(n_components=2, random_state=23).fit_transform(X_popt)\n",
    "    # K-means predictions\n",
    "    kmeans = KMeans(n_clusters=k, random_state=23, n_init=10).fit(X_popt)\n",
    "    y_pred = kmeans.predict(X_popt)\n",
    "    \n",
    "    # match predicted cluster labels with true cluster labels\n",
    "    y_pred, _ = cluster_matching(y_true, y_pred)\n",
    "    # compute accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # plot embeddings\n",
    "    plot_comparison(X_emb, y_true, y_pred, k, method, accuracy)\n",
    "\n",
    "    print('method: {} - p: {} - matching accuracy: {:.2f}'.format(method, p_opt, accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#adebad;\">\n",
    "Visually or quantitatively, in a clever and convincing way, show that the K-MEANS generated clusters overlap with the ground truth clusters (member affiliations). How can we measure the overlapping of the predicted and true clusters?\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"background-color:#F9E9C6;\"> \n",
    "Above we consider k-means clustering for the <b>denoised</b> embedded vectors for the Dijkstra as well as Hamming distance using the true number of clusters (departments) k=42. On the left side, we then color the embedded datapoints according to the groundtruth and on the right diagram according to the predictions, whereas as cluster labels are matched accordingly. Additionally, we calculate the accuracy of our cluster predictions.\n",
    "    \n",
    "Visually the clustering predicted by Dijkstra distance seems more accurate than for the Hamming distance. For example the left purple cluster for the Hamming distance, is predicted as (part of) three clusters. Additionally, Hamming also predicts a large cluster in the center, which is not present in the groundtruth.\n",
    "The poorer visual match for Hamming distance vs. Dijkstra distance is also confirmed by a lower accuracy of $0.39$ vs. $0.56$ for Dijkstra. This also confirms our observations of the TSNE-embeddings for various k's above, where the clustering for Dijkstra looked more \"promising\" for larger $k$ than for Hamming distance. As such, Dijkstra distance seems to be more informative for the relation (belonging to the same department) considered here. However, both accuracies are considerably higher than for random guessing ($\\frac{1}{42}$ if clusters were even).\n",
    "    \n",
    "Although, these figures need to be properly validated, we conclude that CSE clustering - particularly in combination with Dijkstra distance - seems to perform quite well when compared to the groundtruth in this particular task.\n",
    "\n",
    "</u1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write here your explanations, observation and thoughts about results of the experiments above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Comments\n",
    "\n",
    "We hope you found this exercise instructive.\n",
    "\n",
    "Feel free to leave comments below, we will read them carefully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slt-ce",
   "language": "python",
   "name": "slt-ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
